Amir:
- Added background research in relevant parts of the file
- Created Q-Network architecture, Agent and ReplayBuffer 
- Created GIF function to monitor the progress of the LunarLander every 100 episodes
- Exported the DQN model to path in 'saved_models' folder
- Created testing code where render mode='human'
- Used wandb (Weights and Biases) to log the progress of Reward Sum, Average Reward, Epsilon and Number of time steps 
- Just in case code fails, created a train and test.py file with the same architecture to train the agent and test it separately [Failsafe]

Justin:
- Added background research in relevant parts of the file
- Did hyperparameter tuning, experimenting with different values for the following parameters: 
    - Network Architecture
    - Learning Rate
    - Discount Factor
    - Epsilon Decay Rate
    - Replay Buffer Size
    - Target Network Update Frequency
- Coded Baseline Code with architecture and Stupid Baseline to test out the gym environment without any architecture
- Did slides for presentation






