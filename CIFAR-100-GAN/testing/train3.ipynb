{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# https://github.com/sh2439/Reinforcement-Learning-Pytorch/blob/master/Lunar-Lander/LunarLander-Pytorch.ipynb\n",
    "\n",
    "# Basic packages\n",
    "import os\n",
    "\n",
    "import shutil\n",
    "\n",
    "from collections import deque\n",
    "\n",
    "from copy import deepcopy\n",
    "\n",
    "from tqdm.auto import tqdm\n",
    "\n",
    "import numpy as np\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "\n",
    "# Lunar lander and RL-GLUE packages\n",
    "# from rl_glue import RLGlue\n",
    "# from environment import BaseEnvironment\n",
    "\n",
    "# from lunar_lander import LunarLanderEnvironment\n",
    "\n",
    "# from agent import BaseAgent\n",
    "\n",
    "# from plot_script import plot_result, draw_neural_net\n",
    "\n",
    "# Pytorch packages\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.autograd import Variable\n",
    "import torch.nn.functional as F\n",
    "\n",
    "# Gym packages\n",
    "import gym\n",
    "from gym import wrappers\n",
    "from time import time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# def draw_neural_net(ax, left, right, bottom, top, layer_sizes, coefs_, intercepts_, n_iter_, loss_):\n",
    "#     '''\n",
    "#     Draw a neural network cartoon using matplotilb.\n",
    "    \n",
    "#     :usage:\n",
    "#         >>> fig = plt.figure(figsize=(12, 12))\n",
    "#         >>> draw_neural_net(fig.gca(), .1, .9, .1, .9, [4, 7, 2])\n",
    "    \n",
    "#     :parameters:\n",
    "#         - ax : matplotlib.axes.AxesSubplot\n",
    "#             The axes on which to plot the cartoon (get e.g. by plt.gca())\n",
    "#         - left : float\n",
    "#             The center of the leftmost node(s) will be placed here\n",
    "#         - right : float\n",
    "#             The center of the rightmost node(s) will be placed here\n",
    "#         - bottom : float\n",
    "#             The center of the bottommost node(s) will be placed here\n",
    "#         - top : float\n",
    "#             The center of the topmost node(s) will be placed here\n",
    "#         - layer_sizes : list of int\n",
    "#             List of layer sizes, including input and output dimensionality\n",
    "#     '''\n",
    "#     n_layers = len(layer_sizes)\n",
    "#     v_spacing = (top - bottom)/float(max(layer_sizes))\n",
    "#     h_spacing = (right - left)/float(len(layer_sizes) - 1)\n",
    "    \n",
    "#     # Input-Arrows\n",
    "#     layer_top_0 = v_spacing*(layer_sizes[0] - 1)/2. + (top + bottom)/2.\n",
    "#     for m in xrange(layer_sizes[0]):\n",
    "#         plt.arrow(left-0.18, layer_top_0 - m*v_spacing, 0.12, 0,  lw =1, head_width=0.01, head_length=0.02)\n",
    "    \n",
    "#     # Nodes\n",
    "#     for n, layer_size in enumerate(layer_sizes):\n",
    "#         layer_top = v_spacing*(layer_size - 1)/2. + (top + bottom)/2.\n",
    "#         for m in xrange(layer_size):\n",
    "#             circle = plt.Circle((n*h_spacing + left, layer_top - m*v_spacing), v_spacing/8.,\n",
    "#                                 color='w', ec='k', zorder=4)\n",
    "#             if n == 0:\n",
    "#                 plt.text(left-0.125, layer_top - m*v_spacing, r'$X_{'+str(m+1)+'}$', fontsize=15)\n",
    "#             elif (n_layers == 3) & (n == 1):\n",
    "#                 plt.text(n*h_spacing + left+0.00, layer_top - m*v_spacing+ (v_spacing/8.+0.01*v_spacing), r'$H_{'+str(m+1)+'}$', fontsize=15)\n",
    "#             elif n == n_layers -1:\n",
    "#                 plt.text(n*h_spacing + left+0.10, layer_top - m*v_spacing, r'$y_{'+str(m+1)+'}$', fontsize=15)\n",
    "#             ax.add_artist(circle)\n",
    "#     # Bias-Nodes\n",
    "#     for n, layer_size in enumerate(layer_sizes):\n",
    "#         if n < n_layers -1:\n",
    "#             x_bias = (n+0.5)*h_spacing + left\n",
    "#             y_bias = top + 0.005\n",
    "#             circle = plt.Circle((x_bias, y_bias), v_spacing/8., color='w', ec='k', zorder=4)\n",
    "#             plt.text(x_bias-(v_spacing/8.+0.10*v_spacing+0.01), y_bias, r'$1$', fontsize=15)\n",
    "#             ax.add_artist(circle)   \n",
    "#     # Edges\n",
    "#     # Edges between nodes\n",
    "#     for n, (layer_size_a, layer_size_b) in enumerate(zip(layer_sizes[:-1], layer_sizes[1:])):\n",
    "#         layer_top_a = v_spacing*(layer_size_a - 1)/2. + (top + bottom)/2.\n",
    "#         layer_top_b = v_spacing*(layer_size_b - 1)/2. + (top + bottom)/2.\n",
    "#         for m in xrange(layer_size_a):\n",
    "#             for o in xrange(layer_size_b):\n",
    "#                 line = plt.Line2D([n*h_spacing + left, (n + 1)*h_spacing + left],\n",
    "#                                   [layer_top_a - m*v_spacing, layer_top_b - o*v_spacing], c='k')\n",
    "#                 ax.add_artist(line)\n",
    "#                 xm = (n*h_spacing + left)\n",
    "#                 xo = ((n + 1)*h_spacing + left)\n",
    "#                 ym = (layer_top_a - m*v_spacing)\n",
    "#                 yo = (layer_top_b - o*v_spacing)\n",
    "#                 rot_mo_rad = np.arctan((yo-ym)/(xo-xm))\n",
    "#                 rot_mo_deg = rot_mo_rad*180./np.pi\n",
    "#                 xm1 = xm + (v_spacing/8.+0.05)*np.cos(rot_mo_rad)\n",
    "#                 if n == 0:\n",
    "#                     if yo > ym:\n",
    "#                         ym1 = ym + (v_spacing/8.+0.12)*np.sin(rot_mo_rad)\n",
    "#                     else:\n",
    "#                         ym1 = ym + (v_spacing/8.+0.05)*np.sin(rot_mo_rad)\n",
    "#                 else:\n",
    "#                     if yo > ym:\n",
    "#                         ym1 = ym + (v_spacing/8.+0.12)*np.sin(rot_mo_rad)\n",
    "#                     else:\n",
    "#                         ym1 = ym + (v_spacing/8.+0.04)*np.sin(rot_mo_rad)\n",
    "#                 plt.text( xm1, ym1,\\\n",
    "#                          str(round(coefs_[n][m, o],4)),\\\n",
    "#                          rotation = rot_mo_deg, \\\n",
    "#                          fontsize = 10)\n",
    "#     # Edges between bias and nodes\n",
    "#     for n, (layer_size_a, layer_size_b) in enumerate(zip(layer_sizes[:-1], layer_sizes[1:])):\n",
    "#         if n < n_layers-1:\n",
    "#             layer_top_a = v_spacing*(layer_size_a - 1)/2. + (top + bottom)/2.\n",
    "#             layer_top_b = v_spacing*(layer_size_b - 1)/2. + (top + bottom)/2.\n",
    "#         x_bias = (n+0.5)*h_spacing + left\n",
    "#         y_bias = top + 0.005 \n",
    "#         for o in xrange(layer_size_b):\n",
    "#             line = plt.Line2D([x_bias, (n + 1)*h_spacing + left],\n",
    "#                           [y_bias, layer_top_b - o*v_spacing], c='k')\n",
    "#             ax.add_artist(line)\n",
    "#             xo = ((n + 1)*h_spacing + left)\n",
    "#             yo = (layer_top_b - o*v_spacing)\n",
    "#             rot_bo_rad = np.arctan((yo-y_bias)/(xo-x_bias))\n",
    "#             rot_bo_deg = rot_bo_rad*180./np.pi\n",
    "#             xo2 = xo - (v_spacing/8.+0.01)*np.cos(rot_bo_rad)\n",
    "#             yo2 = yo - (v_spacing/8.+0.01)*np.sin(rot_bo_rad)\n",
    "#             xo1 = xo2 -0.05 *np.cos(rot_bo_rad)\n",
    "#             yo1 = yo2 -0.05 *np.sin(rot_bo_rad)\n",
    "#             plt.text( xo1, yo1,\\\n",
    "#                  str(round(intercepts_[n][o],4)),\\\n",
    "#                  rotation = rot_bo_deg, \\\n",
    "#                  fontsize = 10)    \n",
    "                \n",
    "#     # Output-Arrows\n",
    "#     layer_top_0 = v_spacing*(layer_sizes[-1] - 1)/2. + (top + bottom)/2.\n",
    "#     for m in xrange(layer_sizes[-1]):\n",
    "#         plt.arrow(right+0.015, layer_top_0 - m*v_spacing, 0.16*h_spacing, 0,  lw =1, head_width=0.01, head_length=0.02)\n",
    "#     # Record the n_iter_ and loss\n",
    "#     plt.text(left + (right-left)/3., bottom - 0.005*v_spacing, \\\n",
    "#              'Steps:'+str(n_iter_)+'    Loss: ' + str(round(loss_, 6)), fontsize = 15)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# class RLGlue:\n",
    "#     \"\"\"RLGlue class\n",
    "#     args:\n",
    "#         env_name (string): the name of the module where the Environment class can be found\n",
    "#         agent_name (string): the name of the module where the Agent class can be found\n",
    "#     \"\"\"\n",
    "\n",
    "#     def __init__(self, env_class, agent_class):\n",
    "#         self.environment = env_class()\n",
    "#         self.agent = agent_class()\n",
    "\n",
    "#         self.total_reward = None\n",
    "#         self.last_action = None\n",
    "#         self.num_steps = None\n",
    "#         self.num_episodes = None\n",
    "\n",
    "#     def rl_init(self, agent_init_info={}, env_init_info={}):\n",
    "#         \"\"\"Initial method called when RLGlue experiment is created\"\"\"\n",
    "#         self.environment.env_init(env_init_info)\n",
    "#         self.agent.agent_init(agent_init_info)\n",
    "\n",
    "#         self.total_reward = 0.0\n",
    "#         self.num_steps = 0\n",
    "#         self.num_episodes = 0\n",
    "\n",
    "#     def rl_start(self, agent_start_info={}, env_start_info={}):\n",
    "#         \"\"\"Starts RLGlue experiment\n",
    "#         Returns:\n",
    "#             tuple: (state, action)\n",
    "#         \"\"\"\n",
    "\n",
    "#         last_state = self.environment.env_start()\n",
    "#         self.last_action = self.agent.agent_start(last_state)\n",
    "\n",
    "#         observation = (last_state, self.last_action)\n",
    "\n",
    "#         return observation\n",
    "\n",
    "#     def rl_agent_start(self, observation):\n",
    "#         \"\"\"Starts the agent.\n",
    "#         Args:\n",
    "#             observation: The first observation from the environment\n",
    "#         Returns:\n",
    "#             The action taken by the agent.\n",
    "#         \"\"\"\n",
    "#         return self.agent.agent_start(observation)\n",
    "\n",
    "#     def rl_agent_step(self, reward, observation):\n",
    "#         \"\"\"Step taken by the agent\n",
    "#         Args:\n",
    "#             reward (float): the last reward the agent received for taking the\n",
    "#                 last action.\n",
    "#             observation : the state observation the agent receives from the\n",
    "#                 environment.\n",
    "#         Returns:\n",
    "#             The action taken by the agent.\n",
    "#         \"\"\"\n",
    "#         return self.agent.agent_step(reward, observation)\n",
    "\n",
    "#     def rl_agent_end(self, reward):\n",
    "#         \"\"\"Run when the agent terminates\n",
    "#         Args:\n",
    "#             reward (float): the reward the agent received when terminating\n",
    "#         \"\"\"\n",
    "#         self.agent.agent_end(reward)\n",
    "\n",
    "#     def rl_env_start(self):\n",
    "#         \"\"\"Starts RL-Glue environment.\n",
    "#         Returns:\n",
    "#             (float, state, Boolean): reward, state observation, boolean\n",
    "#                 indicating termination\n",
    "#         \"\"\"\n",
    "#         self.total_reward = 0.0\n",
    "#         self.num_steps = 1\n",
    "\n",
    "#         this_observation = self.environment.env_start()\n",
    "\n",
    "#         return this_observation\n",
    "\n",
    "#     def rl_env_step(self, action):\n",
    "#         \"\"\"Step taken by the environment based on action from agent\n",
    "#         Args:\n",
    "#             action: Action taken by agent.\n",
    "#         Returns:\n",
    "#             (float, state, Boolean): reward, state observation, boolean\n",
    "#                 indicating termination.\n",
    "#         \"\"\"\n",
    "#         ro = self.environment.env_step(action)\n",
    "#         (this_reward, _, terminal) = ro\n",
    "\n",
    "#         self.total_reward += this_reward\n",
    "\n",
    "#         if terminal:\n",
    "#             self.num_episodes += 1\n",
    "#         else:\n",
    "#             self.num_steps += 1\n",
    "\n",
    "#         return ro\n",
    "\n",
    "#     def rl_step(self):\n",
    "#         \"\"\"Step taken by RLGlue, takes environment step and either step or\n",
    "#             end by agent.\n",
    "#         Returns:\n",
    "#             (float, state, action, Boolean): reward, last state observation,\n",
    "#                 last action, boolean indicating termination\n",
    "#         \"\"\"\n",
    "\n",
    "#         (reward, last_state, term) = self.environment.env_step(self.last_action)\n",
    "\n",
    "#         self.total_reward += reward\n",
    "\n",
    "#         if term:\n",
    "#             self.num_episodes += 1\n",
    "#             self.agent.agent_end(reward)\n",
    "#             roat = (reward, last_state, None, term)\n",
    "#         else:\n",
    "#             self.num_steps += 1\n",
    "#             self.last_action = self.agent.agent_step(reward, last_state)\n",
    "#             roat = (reward, last_state, self.last_action, term)\n",
    "\n",
    "#         return roat\n",
    "\n",
    "#     def rl_cleanup(self):\n",
    "#         \"\"\"Cleanup done at end of experiment.\"\"\"\n",
    "#         self.environment.env_cleanup()\n",
    "#         self.agent.agent_cleanup()\n",
    "\n",
    "#     def rl_agent_message(self, message):\n",
    "#         \"\"\"Message passed to communicate with agent during experiment\n",
    "#         Args:\n",
    "#             message: the message (or question) to send to the agent\n",
    "#         Returns:\n",
    "#             The message back (or answer) from the agent\n",
    "#         \"\"\"\n",
    "\n",
    "#         return self.agent.agent_message(message)\n",
    "\n",
    "#     def rl_env_message(self, message):\n",
    "#         \"\"\"Message passed to communicate with environment during experiment\n",
    "#         Args:\n",
    "#             message: the message (or question) to send to the environment\n",
    "#         Returns:\n",
    "#             The message back (or answer) from the environment\n",
    "#         \"\"\"\n",
    "#         return self.environment.env_message(message)\n",
    "\n",
    "#     def rl_episode(self, max_steps_this_episode):\n",
    "#         \"\"\"Runs an RLGlue episode\n",
    "#         Args:\n",
    "#             max_steps_this_episode (Int): the maximum steps for the experiment to run in an episode\n",
    "#         Returns:\n",
    "#             Boolean: if the episode should terminate\n",
    "#         \"\"\"\n",
    "#         is_terminal = False\n",
    "\n",
    "#         self.rl_start()\n",
    "\n",
    "#         while (not is_terminal) and ((max_steps_this_episode == 0) or\n",
    "#                                      (self.num_steps < max_steps_this_episode)):\n",
    "#             rl_step_result = self.rl_step()\n",
    "#             is_terminal = rl_step_result[3]\n",
    "\n",
    "#         return is_terminal\n",
    "\n",
    "#     def rl_return(self):\n",
    "#         \"\"\"The total reward\n",
    "#         Returns:\n",
    "#             float: the total reward\n",
    "#         \"\"\"\n",
    "#         return self.total_reward\n",
    "\n",
    "#     def rl_num_steps(self):\n",
    "#         \"\"\"The total number of steps taken\n",
    "#         Returns:\n",
    "#             Int: the total number of steps taken\n",
    "#         \"\"\"\n",
    "#         return self.num_steps\n",
    "\n",
    "#     def rl_num_episodes(self):\n",
    "#         \"\"\"The number of episodes\n",
    "#         Returns\n",
    "#             Int: the total number of episodes\n",
    "#         \"\"\"\n",
    "#         return self.num_episodes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "class RLModel(nn.Module):\n",
    "    \n",
    "    def __init__(self, network_arch):\n",
    "        \n",
    "        super().__init__()\n",
    "        self.num_states = network_arch['num_states']\n",
    "        self.hidden_units = network_arch['num_hidden_units']\n",
    "        self.num_actions = network_arch['num_actions']\n",
    "        \n",
    "        # The hidden layer\n",
    "        self.fc1 = nn.Linear(in_features = self.num_states, out_features = self.hidden_units)\n",
    "        \n",
    "        # The output layer\n",
    "        self.fc2 = nn.Linear(in_features = self.hidden_units, out_features = self.num_actions)\n",
    "        \n",
    "    def forward(self, x):\n",
    "        \n",
    "        x = F.relu(self.fc1(x))\n",
    "        \n",
    "        # No activation func, output should be a tensor(batch, num_actions)\n",
    "        out = self.fc2(x)\n",
    "        \n",
    "        return out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "RLModel(\n",
       "  (fc1): Linear(in_features=8, out_features=20, bias=True)\n",
       "  (fc2): Linear(in_features=20, out_features=3, bias=True)\n",
       ")"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "network_arch = {\n",
    "    'num_states' : 8,\n",
    "    'num_hidden_units': 20,\n",
    "    'num_actions':3\n",
    "    \n",
    "}\n",
    "\n",
    "model = RLModel(network_arch)\n",
    "\n",
    "model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'draw_neural_net' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[8], line 5\u001b[0m\n\u001b[0;32m      3\u001b[0m ax \u001b[38;5;241m=\u001b[39m fig\u001b[38;5;241m.\u001b[39mgca()\n\u001b[0;32m      4\u001b[0m ax\u001b[38;5;241m.\u001b[39maxis(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124moff\u001b[39m\u001b[38;5;124m'\u001b[39m)\n\u001b[1;32m----> 5\u001b[0m \u001b[43mdraw_neural_net\u001b[49m(ax, \u001b[38;5;241m.1\u001b[39m, \u001b[38;5;241m.9\u001b[39m, \u001b[38;5;241m.1\u001b[39m, \u001b[38;5;241m.9\u001b[39m, [\u001b[38;5;241m8\u001b[39m, \u001b[38;5;241m20\u001b[39m, \u001b[38;5;241m4\u001b[39m])\n",
      "\u001b[1;31mNameError\u001b[0m: name 'draw_neural_net' is not defined"
     ]
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 800x800 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "\n",
    "fig = plt.figure(figsize = (8,8))\n",
    "fig.suptitle('Neural Network Archiecture')\n",
    "ax = fig.gca()\n",
    "ax.axis('off')\n",
    "draw_neural_net(ax, .1, .9, .1, .9, [8, 20, 4])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Set the learning rate\n",
    "learning_rate = 0.001\n",
    "\n",
    "# We will instantiate the optimizer in the agent class.\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr = learning_rate, betas = [0.99, 0.999], eps = 1e-04)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "criterion = nn.MSELoss()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Buffer:\n",
    "    \n",
    "    def __init__(self, batch_size, buffer_size, seed):\n",
    "        \n",
    "        self.buffer_size = buffer_size\n",
    "        self.batch_size = batch_size\n",
    "        self.rand_generator = np.random.RandomState(seed)\n",
    "        \n",
    "        self.buffer = []\n",
    "        \n",
    "        \n",
    "    def append(self, state, action, terminal, reward, next_state):\n",
    "        \"\"\"\n",
    "        Append the next experience.\n",
    "        \n",
    "        Args:\n",
    "            state: the state (torch tensor).\n",
    "            action: the action (integer).\n",
    "            terminal: 1 if the next state is the terminal, 0 otherwise.\n",
    "        \n",
    "        \"\"\"\n",
    "        # delete the first experience if the size is reaching the maximum\n",
    "        if len(self.buffer) == self.buffer_size:\n",
    "            del self.buffer[0]\n",
    "            \n",
    "        self.buffer.append((torch.tensor(state), torch.tensor(action), torch.tensor(terminal), torch.tensor(reward).float(), torch.tensor(next_state)))\n",
    "        \n",
    "    def sample(self):\n",
    "        \"\"\"\n",
    "        Sample from the buffer and return the virtual experience.\n",
    "        \n",
    "        Args:\n",
    "            None\n",
    "        Returns:\n",
    "            A list of transition tuples (state, action, terminal, reward, next_state), list length: batch_size\n",
    "        \"\"\"\n",
    "        \n",
    "        indexs = self.rand_generator.choice(len(self.buffer), size = self.batch_size)\n",
    "        \n",
    "        transitions = [self.buffer[idx] for idx in indexs]\n",
    "        \n",
    "        return transitions\n",
    "       \n",
    "    def get_buffer(self):\n",
    "        \"\"\"\n",
    "        Return the current buffer\n",
    "        \"\"\"\n",
    "        return self.buffer\n",
    "        \n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def softmax(action_values, tau = 1.0):\n",
    "    \n",
    "    \"\"\"\n",
    "    Args:\n",
    "        action_values: A torch tensor (2d) of size (batch_size, num_actions).\n",
    "        tau: Tempearture parameter.\n",
    "    \n",
    "    Returns:\n",
    "        probs: A torch tensor of size (batch_size, num_actions). The value represents the probability of select \n",
    "        that action.\n",
    "    \"\"\"\n",
    "    \n",
    "    max_action_value = torch.max(action_values, axis = 1, keepdim = True)[0]/tau\n",
    "    action_values = action_values/tau\n",
    "    \n",
    "    preference = action_values - max_action_value\n",
    "    \n",
    "    exp_action = torch.exp(preference)\n",
    "    sum_exp_action = torch.sum(exp_action, axis = 1).view(-1,1)\n",
    "\n",
    "\n",
    "    probs = exp_action/sum_exp_action\n",
    "\n",
    "    \n",
    "    return probs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_network(experiences, model, current_model, optimizer, criterion, discount, tau):\n",
    "    \n",
    "    \"\"\"\n",
    "    Calculate the TD-error and update the network\n",
    "    \n",
    "    \"\"\"\n",
    "    optimizer.zero_grad()\n",
    "    \n",
    "    states, actions, terminals, rewards, next_states = map(list, zip(*experiences))\n",
    "    \n",
    "    \n",
    "#     print(next_states)\n",
    "    q_next = current_model(Variable(torch.stack(next_states))).squeeze()\n",
    "    probs = softmax(q_next, tau)\n",
    "\n",
    "    # calculate the maximum action value of next states\n",
    "#     expected_q_next = (1-torch.stack(terminals)) * (torch.sum(probs * q_next , axis = 1))\n",
    "    max_q_next = (1-torch.stack(terminals)) * (torch.max(q_next , axis = 1)[0])\n",
    "    # calculate the targets\n",
    "    \n",
    "    rewards = torch.stack(rewards).float()\n",
    "#     targets = Variable(rewards + (discount * expected_q_next)).float()\n",
    "    targets = Variable(rewards + (discount * max_q_next)).float()\n",
    "    \n",
    "    # calculate the outputs from the previous states (batch_size, num_actions)\n",
    "    outputs = model(Variable(torch.stack(states).float())).squeeze()\n",
    "    \n",
    "\n",
    "    actions = torch.stack(actions).view(-1,1)\n",
    "    \n",
    "    outputs = torch.gather(outputs, 1, actions).squeeze()\n",
    "    \n",
    "    # the loss\n",
    "    loss = criterion(outputs, targets)\n",
    "    \n",
    "    loss.backward()\n",
    "    \n",
    "    # update\n",
    "    optimizer.step()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'BaseAgent' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[27], line 1\u001b[0m\n\u001b[1;32m----> 1\u001b[0m \u001b[38;5;28;01mclass\u001b[39;00m \u001b[38;5;21;01mAgent\u001b[39;00m(\u001b[43mBaseAgent\u001b[49m):\n\u001b[0;32m      3\u001b[0m     \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m__init__\u001b[39m(\u001b[38;5;28mself\u001b[39m):\n\u001b[0;32m      4\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mname \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mLunar Lander Expected Sarsa Agent\u001b[39m\u001b[38;5;124m'\u001b[39m\n",
      "\u001b[1;31mNameError\u001b[0m: name 'BaseAgent' is not defined"
     ]
    }
   ],
   "source": [
    "class Agent(BaseAgent):\n",
    "    \n",
    "    def __init__(self):\n",
    "        self.name = 'Lunar Lander Expected Sarsa Agent'\n",
    "        \n",
    "    def agent_init(self, agent_config):\n",
    "        \n",
    "        \"\"\"\n",
    "        Called when the experiment first starts.\n",
    "        Args:\n",
    "            agent_config: Python dict contains:\n",
    "                        {\n",
    "                        network_arch: dict,\n",
    "                        batch_size: integer,\n",
    "                        buffer_size: integer,\n",
    "                        gamma: float,\n",
    "                        learning_rate: float,\n",
    "                        tau: float,\n",
    "                        seed:integer,\n",
    "                        num_replay_updates: float\n",
    "                        \n",
    "                        \n",
    "                        }\n",
    "        \n",
    "        \"\"\"\n",
    "        # The model\n",
    "        self.model = RLModel(agent_config['network_arch'])\n",
    "        # The replay buffer\n",
    "        self.buffer = Buffer(agent_config['batch_size'],\n",
    "                            agent_config['buffer_size'],\n",
    "                            agent_config['seed'])\n",
    "        # The optimizer\n",
    "        self.optimizer = torch.optim.Adam(self.model.parameters(), \n",
    "                                          lr = agent_config['learning_rate'], \n",
    "                                          betas = [0.99,0.999], \n",
    "                                          eps = 1e-04)\n",
    "        # The loss\n",
    "        self.criterion = nn.MSELoss()\n",
    "        \n",
    "        self.batch_size = agent_config['batch_size']\n",
    "        self.discount = agent_config['gamma']\n",
    "        self.tau = agent_config['tau']\n",
    "        self.num_replay = agent_config['num_replay_updates']\n",
    "        self.num_actions = agent_config['network_arch']['num_actions']\n",
    "        # random number generator\n",
    "        self.rand_generator = np.random.RandomState(agent_config['seed'])\n",
    "        \n",
    "        \n",
    "        self.last_state = None\n",
    "        self.last_action = None\n",
    "        \n",
    "        self.sum_rewards = 0\n",
    "        self.episode_steps = 0\n",
    "        \n",
    "    def policy(self, state):\n",
    "        \n",
    "        \"\"\"\n",
    "        Select the action given a single state.\n",
    "        \n",
    "        \"\"\"\n",
    "        # compute action values states:(1, state_dim)\n",
    "        q_values = self.model(state)\n",
    "\n",
    "        # compute the probs of each action (1, num_actions)\n",
    "        probs = softmax(q_values.data, self.tau)\n",
    "        probs = np.array(probs)\n",
    "        probs /= probs.sum()\n",
    "\n",
    "        # select action\n",
    "        action = self.rand_generator.choice(self.num_actions, 1, p = probs.squeeze())\n",
    "        \n",
    "        return action\n",
    "    \n",
    "    \n",
    "    def agent_start(self, state):\n",
    "        \"\"\"\n",
    "        Called when the experiments starts, after the env starts.\n",
    "        \n",
    "        Args:\n",
    "            state: pytorch tensor.\n",
    "            \n",
    "        Returns:\n",
    "            action: The first action.\n",
    "        \"\"\"\n",
    "        \n",
    "        self.sum_rewards = 0\n",
    "        self.episode_steps = 0\n",
    "        \n",
    "        state = torch.tensor([state]).view(1, -1)\n",
    "\n",
    "        action = self.policy(state)\n",
    "        \n",
    "        self.last_state = state\n",
    "        self.last_action = int(action)\n",
    "        \n",
    "        return self.last_action\n",
    "    \n",
    "    def agent_step(self, reward, state):\n",
    "        \n",
    "        \"\"\"\n",
    "        The agent takes one step.\n",
    "        \n",
    "        Args:\n",
    "            reward: The reward the agent received, float.\n",
    "            state: The next state the agent received, Numpy array.\n",
    "            \n",
    "        Returns:\n",
    "            action: The action the agent is taking, integer.\n",
    "        \n",
    "        \"\"\"\n",
    "        ### Add another step and reward\n",
    "        self.episode_steps += 1\n",
    "        self.sum_rewards += reward\n",
    "        \n",
    "        ### Select action\n",
    "        state = torch.tensor([state])\n",
    "        \n",
    "        action = self.policy(state)\n",
    "        \n",
    "        ### Append new experience to the buffer\n",
    "        self.buffer.append(self.last_state, self.last_action, 0, reward, state)\n",
    "        \n",
    "        ### Replay steps:\n",
    "        # replay only if the buffer size is large enough\n",
    "        if len(self.buffer.get_buffer()) >= self.batch_size:\n",
    "            # copy the current network\n",
    "            current_model = deepcopy(self.model)\n",
    "            \n",
    "            # replay steps:\n",
    "            for i in range(self.num_replay):\n",
    "                \n",
    "                # sample experiences from the buffer\n",
    "                experiences = self.buffer.sample()\n",
    "                \n",
    "                # train the network\n",
    "                train_network(experiences, self.model, current_model, self.optimizer, self.criterion, self.discount, self.tau)\n",
    "                \n",
    "        ### Update the last state and action\n",
    "        self.last_state = state\n",
    "        self.last_action = int(action)\n",
    "        \n",
    "        return self.last_action\n",
    "    \n",
    "    def agent_end(self, reward):\n",
    "        \n",
    "        \"\"\"\n",
    "        Called when the agent terminates.\n",
    "        \n",
    "        Args:\n",
    "            reward: The reward the agent received for the termination.\n",
    "        \n",
    "        \"\"\"\n",
    "        self.episode_steps += 1\n",
    "        self.sum_rewards += reward\n",
    "        \n",
    "        ### Find the final state\n",
    "        state = torch.zeros_like(self.last_state)\n",
    "        \n",
    "        ### Append new experience to the buffer\n",
    "        self.buffer.append(self.last_state, self.last_action, 1, reward, state)\n",
    "        \n",
    "        \n",
    "        ### Replay steps:\n",
    "        # replay only if the buffer size is large enough\n",
    "        if len(self.buffer.get_buffer()) >= self.batch_size:\n",
    "            # copy the current network\n",
    "            current_model = deepcopy(self.model)\n",
    "            \n",
    "            # replay steps:\n",
    "            for i in range(self.num_replay):\n",
    "                \n",
    "                # sample experiences from the buffer\n",
    "                experiences = self.buffer.sample()\n",
    "                \n",
    "                # train the network\n",
    "                train_network(experiences, self.model, current_model, self.optimizer, self.criterion, self.discount, self.tau)\n",
    "\n",
    "        ### Save the model at each episode\n",
    "        \n",
    "#         torch.save(self.model, 'new_results/current_nodel.pth')\n",
    "    def agent_message(self, message):\n",
    "        \n",
    "        \"\"\"\n",
    "        Return the given agent message.\n",
    "        \n",
    "        Args:\n",
    "            message: String \n",
    "        \n",
    "       \"\"\"\n",
    "        if message == 'get_sum_reward':\n",
    "            \n",
    "            return self.sum_rewards\n",
    "        else:\n",
    "            raise Exception('No given message of the agent!')\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### A func to run the experiment\n",
    "\n",
    "def run_experiment(environment, agent, environment_configs, agent_configs, experiment_configs, finetune, PATH = 'new_results/current_model_700.pth'):\n",
    "    \n",
    "    \"\"\"\n",
    "    Run the experiment.\n",
    "    \n",
    "    Args:\n",
    "        envierment: The environment class.\n",
    "        agent: The agent class.\n",
    "        environment_configs: environment parameters (Python dict).\n",
    "        agent_configs agent parameters (Python dict).\n",
    "        {\n",
    "        \n",
    "        \n",
    "        }\n",
    "\n",
    "        experiment_configs: experiment parameters (Python dict)\n",
    "        {'num_runs': Integer. Number of runs of the experiment,\n",
    "        'num_episodes': Integer. Number of episodes of the experiment,\n",
    "        'timeout': Integer. Time step limit of the experiment.\n",
    "        }\n",
    "        \n",
    "        finetune: boolean. Finetune or train from scratch.\n",
    "    Returns:\n",
    "        agent_sum_reward: Numpy array(num_runs, num_episodes), the sum reward received by the agent.\n",
    "    \n",
    "    \"\"\"\n",
    "    \n",
    "    ### Instantiate the RLGlue class\n",
    "    rl_glue = RLGlue(environment, agent)\n",
    "    \n",
    "    ### Save sum of reward\n",
    "    agent_sum_reward = np.zeros((experiment_configs['num_runs'],experiment_configs['num_episodes']))\n",
    "    \n",
    "    ### Loop over runs\n",
    "    for run in tqdm(range(experiment_configs['num_runs'])):\n",
    "        \n",
    "        # Set the random seed for agent and environment\n",
    "        agent_configs['seed'] = run\n",
    "        environment_configs['seed'] = run\n",
    "        \n",
    "        # Initialize the rl_glue\n",
    "        rl_glue.rl_init(agent_configs, environment_configs)\n",
    "        \n",
    "        # Finetuning\n",
    "        if finetune:\n",
    "            \n",
    "            checkpoint = torch.load(PATH)\n",
    "            rl_glue.agent.model.load_state_dict(checkpoint['model_state_dict'])\n",
    "\n",
    "            start_episode = checkpoint['episode'] + 1\n",
    "            \n",
    "            print('Finetuning...')\n",
    "        else:\n",
    "            start_episode = 0\n",
    "            print('Training...')\n",
    "        \n",
    "        ### Loop over episodes\n",
    "        for episode in tqdm(range(start_episode, start_episode + experiment_configs['num_episodes'])):\n",
    "            # Run episode\n",
    "            rl_glue.rl_episode(experiment_configs['timeout'])\n",
    "            \n",
    "            # Get reward\n",
    "            episode_reward = rl_glue.rl_agent_message('get_sum_reward')\n",
    "            \n",
    "            # Save the reward in the array\n",
    "            agent_sum_reward[run, episode - start_episode] = episode_reward\n",
    "            \n",
    "            # Save the model for testing\n",
    "            if episode == start_episode + experiment_configs['num_episodes'] - 1:\n",
    "                \n",
    "                current_model = rl_glue.agent.model\n",
    "                torch.save({'episode':episode,\n",
    "                'model_state_dict':current_model.state_dict(),\n",
    "                            },\n",
    "                'new_results2/current_model_{}.pth'.format(episode+1))\n",
    "                \n",
    "            print('Run:{}, episode:{}, reward:{}'.format(run, episode, episode_reward))\n",
    "            \n",
    "    return agent_sum_reward\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_reward(rewards, title, starting_episode = 0):\n",
    "    \"\"\"Plot the reward of each episode.\n",
    "    \"\"\"\n",
    "    \n",
    "    rewards = np.mean(rewards, axis = 0).squeeze()\n",
    "    episodes = np.arange(starting_episode, starting_episode + rewards.shape[0], 1)\n",
    "    \n",
    "    plt.figure(figsize = (20,10))\n",
    "    plt.title(title)\n",
    "    plt.xlabel('Episode')\n",
    "    plt.ylabel('Reward of each episode')\n",
    "    plt.plot(episodes, rewards)\n",
    "    plt.show()\n",
    "    \n",
    "    return\n",
    "\n",
    "\n",
    "def smooth(data, k):\n",
    "    \"\"\"\n",
    "    Smooth the data with moving average.\n",
    "    \n",
    "    \"\"\"\n",
    "    num_episodes = data.shape[1]\n",
    "    num_runs = data.shape[0]\n",
    "\n",
    "    smoothed_data = np.zeros((num_runs, num_episodes))\n",
    "\n",
    "    for i in range(num_episodes):\n",
    "        if i < k:\n",
    "            smoothed_data[:, i] = np.mean(data[:, :i+1], axis = 1)   \n",
    "        else:\n",
    "            smoothed_data[:, i] = np.mean(data[:, i-k:i+1], axis = 1)    \n",
    "        \n",
    "\n",
    "    return smoothed_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "agent = Agent\n",
    "environment = LunarLanderEnvironment\n",
    "\n",
    "environment_configs = {}\n",
    "agent_configs = {\n",
    "    'network_arch' : {'num_states':8,\n",
    "               'num_hidden_units' : 256,\n",
    "               'num_actions': 4},\n",
    "    \n",
    "    'batch_size': 8,\n",
    "    'buffer_size': 50000,\n",
    "    'gamma': 0.99,\n",
    "    'learning_rate': 1e-4,\n",
    "    'tau':0.01 ,\n",
    "    'seed':0,\n",
    "    'num_replay_updates':5\n",
    "      \n",
    "}\n",
    "\n",
    "experiment_configs = {\n",
    "    'num_runs':1,\n",
    "    'num_episodes':100,\n",
    "    'timeout': 1000\n",
    "}\n",
    "\n",
    "PATH = 'new_results/current_model_700.pth'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sum_reward = run_experiment(environment, agent, environment_configs, agent_configs, experiment_configs, finetune = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "smoothed_reward = smooth(sum_reward, 25)\n",
    "\n",
    "\n",
    "plot_reward(sum_reward, 'Reward of each episode (unsmoothed data)', 700)\n",
    "\n",
    "plot_reward(smoothed_reward, 'Smoothed reward', 700)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_path = 'new_results2/current_model_800.pth'\n",
    "current_model = RLModel(agent_configs['network_arch'])\n",
    "checkpoint = torch.load(model_path)\n",
    "current_model.load_state_dict(checkpoint['model_state_dict'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def policy(state, model, num_actions = 4):\n",
    "        \n",
    "        \"\"\"\n",
    "        Select the action given a single state.\n",
    "        \n",
    "        \"\"\"\n",
    "        model.eval()\n",
    "        # compute action values states:(1, state_dim)\n",
    "        q_values = model(state)\n",
    "\n",
    "        # compute the probs of each action (1, num_actions)\n",
    "        probs = softmax(q_values.data, tau = 0.01)\n",
    "        probs = np.array(probs)\n",
    "        probs /= probs.sum()\n",
    "\n",
    "        # select action\n",
    "        rand_generator = np.random.RandomState(seed = 1)\n",
    "        action = rand_generator.choice(num_actions, 1, p = probs.squeeze())\n",
    "#         action = np.argmax(probs.squeeze())\n",
    "        \n",
    "        return action"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "env = gym.make(\"LunarLander-v2\")\n",
    "env = wrappers.Monitor(env, './videos_800/' + '/')\n",
    "for i_episode in range(10):\n",
    "    observation = env.reset()\n",
    "    total_reward = 0\n",
    "    \n",
    "    for t in range(1000):\n",
    "        env.render()\n",
    "#         print(observation)\n",
    "        with torch.no_grad():\n",
    "            observation = Variable(torch.tensor(observation).view(1, -1))\n",
    "            action = policy(observation, current_model)\n",
    "            action = int(action.squeeze())\n",
    "\n",
    "            observation, reward, done, info = env.step(action)\n",
    "            total_reward += reward\n",
    "            if done:\n",
    "                \n",
    "                print(\"Episode finished after {} timesteps, total reward : {}\".format(t+1, total_reward))\n",
    "                break\n",
    "env.close()\n",
    "print('end')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.8.15 ('pytorch')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.15"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "da2c18488bdf6f311c73bf4ef7618a8e2f5fdc2961b3bf16c0b895de03986215"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
